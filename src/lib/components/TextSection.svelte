<section class="py-20 px-6 max-w-[1400px] mx-auto grid grid-cols-1 lg:grid-cols-2 gap-12">
  <div class="space-y-2">
    <h2 class="text-4xl lg:text-5xl font-bold leading-none tracking-tight text-white uppercase">
      Transformer-<br>
      Based LLMs<br>
      <span class="text-accent">Don't Make</span><br>
      <span class="text-accent">Good</span> Agents
    </h2>
  </div>
  
  <div class="space-y-6 text-gray-400 text-sm leading-relaxed max-w-lg">
    <p>
      Traditional language models rely on transformer architectures, which excel at pattern recognition and language generation. They are particularly effective for a variety of tasks such as text generation, translation, and summarization.
    </p>
    <p>
      However — when it comes to agentic use cases, situations where the model needs to perform actions, make decisions, or interact with tools — transformer-based LLMs face significant challenges.
    </p>
  </div>
</section>
